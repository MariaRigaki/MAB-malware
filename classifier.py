import os
import sys
import hashlib
import time
import datetime
#from utils import *
from models import *

#from ember import predict_sample
#import lightgbm as lgb
#import numpy as np
import glob
#from torch.autograd import Variable

MALCONV_MODEL_PATH = 'models/malconv/malconv.checkpoint'
#MALCONV2_MODEL_PATH = 'models/example_sd_123.model'
#NONNEG_MODEL_PATH = 'models/nonneg/nonneg.checkpoint'
EMBER_2019_MODEL_PATH = 'models/ember_2019/ember_model.txt'
EMBER_MODEL_gym_PATH = 'models/ember_gym/gradient_boosting.pkl'

rewriter_scan_folder = 'data/share/rewriter/'
minimizer_scan_folder = 'data/share/minimizer/'

def main():
    classifier_name = sys.argv[1]

    print('Model %s loading...' %classifier_name)
    if classifier_name == 'malconv':
        model = MalConvModel( MALCONV_MODEL_PATH, thresh=0.5 )
    #elif classifier_name == 'malconv2':
    #    model = MalConvModel2( MALCONV2_MODEL_PATH, thresh=0.5 )
    #elif classifier_name == 'nonneg_malconv':
    #    model = MalConvModel( NONNEG_MODEL_PATH, thresh=0.35, name='nonneg_malconv' )
    #elif classifier_name == 'ember_gym':
    #    model = EmberModel_gym( EMBER_MODEL_gym_PATH, thresh=0.90 )
    elif classifier_name == 'ember2019':
        model = EmberModel_2019( EMBER_2019_MODEL_PATH, thresh=0.8336 )
    elif classifier_name == 'ember2020':
        model = EmberModel_2020(thresh=0.8336)
    #elif classifier_name == 'clamav':
    #    model = ClamAV()

    count = 0
    count_current = count
    while count < 200:
        if count_current != count:
            print('count: %d' %count)
            count_current = count
        #time.sleep(0.1)
        res1 = evaluate(model, minimizer_scan_folder)
        res2 = evaluate(model, rewriter_scan_folder)
        count += res1 + res2

    ## TODO: for gym only
    ##print('for gym only!!!!')
    #count = 0
    ##count_current = count
    #while count < 100:
    #    #time.sleep(1)
    #    #print(count)
    #    #if count_current != count:
    #    #    #print(count)
    #    #    count_current = count
    #    #print('evaluate Rewriter')
    #    res2 = evaluate(model, minimizer_scan_folder)
    #    res1 = evaluate(model, rewriter_scan_folder)
    #    count += res1 + res2
        
def evaluate(model, classifier_input):
    #print('evaluate %s' %(classifier_input))
    set_benign_files = set(glob.glob(classifier_input + '*.benign'))
    list_file = [x for x in glob.glob(classifier_input + '*') if x not in set_benign_files]

    file_amount = len(list_file)
    #print('================= %d ===================' %file_amount)
    #if file_amount == 0:
    #    time.sleep(1)
    if file_amount > 0:
        list_file.sort(key=os.path.getmtime)
        file_path = list_file[0]
        #print(file_path)
        print('processing %s' %(file_path))
        if os.path.exists(file_path) == False:
            print('file does not exist')
            return 0
        #result = model.predict(file_path)
        evasive = model.is_evasive(file_path)
        if evasive == False:
            print('Malicious! delete!')
            os.system('rm %s' %(file_path))
        else:
            print('#### Benign!')
            os.system('mv %s %s.benign' %(file_path, file_path))
        return 1
    return 0

if __name__ == '__main__':
    main()

#class MalConvModel2(object):
#    def __init__(self, model_path, thresh=0.5, name='malconv2'):
#        print(model_path)
#        self.model = torch.load(model_path)
#        self.model.eval()
#        #weights = torch.load(model_path)#,map_location='cpu')
#        #self.model.load_state_dict( weights['model_state_dict'])
#        self.thresh = thresh
#        self.__name__ = name
#
#    def predict(self, file_path):
#        return self.get_score(file_path) > self.thresh
#
#    def get_score(self, file_path):
#        with open(file_path, 'rb') as fp:
#            bytez = fp.read()
#            print(len(bytez))
#            _inp = torch.from_numpy( np.frombuffer(bytez,dtype=np.uint8)[np.newaxis,:] )
#
#            #exe_input = val_batch_data[0]
#            exe_input = _inp
#            #print(exe_input)
#            exe_input = Variable(exe_input.long(),requires_grad=False)
#            print(exe_input)
#            print(exe_input.shape)
#
#            pred = self.model(exe_input)
#            print(pred)
#            print(pred.shape)
#            #print(pred.detach().numpy()[0,1])
#            print(pred.detach().numpy()[0][0])
#
#            #with torch.no_grad():
#            #    outputs = F.softmax( self.model(_inp), dim=-1)
#            #return outputs.detach().numpy()[0,1]
#            return pred.detach().numpy()[0][0]
#
#def add_score_in_filename(model, classifier_input):
#    print('score %s' %(classifier_input))
#    list_file = glob.glob(classifier_input + '*.exe')
#
#    file_amount = len(list_file)
#    print('================= %d ===================' %file_amount)
#    #if file_amount == 0:
#    #    time.sleep(1)
#    if file_amount > 0:
#        list_file.sort(key=os.path.getmtime)
#        file_path = list_file[0]
#        print('processing %s' %(file_path))
#        if os.path.exists(file_path) == False:
#            print('file does not exist')
#            return
#        score = model.get_score(file_path)
#        print('mv %s %s_%f' %(file_path, file_path, score))
#        os.system('mv %s %s_%f' %(file_path, file_path, score))
#        return True
#    return False
